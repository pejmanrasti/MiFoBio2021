{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "3_unet_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pejmanrasti/MiFoBio2021/blob/main/3_unet_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYmn268qyfzy",
        "outputId": "3dc483eb-fe4c-45e1-8d49-816acbc7a5cc"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxJNtcLHyxK7"
      },
      "source": [
        "## Training a Unet\n",
        "\n",
        "In this notebook, we will train a 2D U-net for nuclei segmentation in the Kaggle Nuclei dataset.\n",
        "\n",
        "It is still possible to do this exercise on the CPU, but you will need some patience to wait for the training. That's why we have added GPU support.\n",
        "Please switch your Notebook to GPU in Edit -> Notebook Settings -> Hardware Accelerator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5CNxOpgx-ey"
      },
      "source": [
        "## The libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK0BTeevRv_N"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFTBWHvqLYwL"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "For this exercise we will be using the Kaggle 2018 Data Science Bowl data again, but this time we will try to segment it with the state of the art network.\n",
        "Let's start with loading the data as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k82ftNngK8Qf"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1EbvS10-83JGNE2nlBxIV42izY1TOr115' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1EbvS10-83JGNE2nlBxIV42izY1TOr115\" -O kaggle_data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -qq kaggle_data.zip && rm kaggle_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89JYxq-bL-Q3"
      },
      "source": [
        "Now make sure that the data was successfully extracted: if everything went fine, you should have folders `nuclei_train_data` and `nuclei_val_data` in your working directory. Check if it is the case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dk-fAyl7L-Zj",
        "outputId": "5341d910-4a17-4db7-aa60-87f6ad8f40fe"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph  kaggle_data.zip\tnuclei_train_data  nuclei_val_data  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuRXDRJsMxVo"
      },
      "source": [
        "__TASK__: Use `ls` to explore the contents of both folders. Running `ls your_folder_name` should display you what is stored in the folder of your interest.\n",
        "\n",
        " How are the images stored? What format do they have? What about the ground truth (the annotation masks)? Which format are they stored in?\n",
        "\n",
        "Hint: you can use the following function to display the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ehovgGj1NQrG"
      },
      "source": [
        "def show_one_image(image_path):\n",
        "  image = imageio.imread(image_path)\n",
        "  plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7WFCKqDUGgC"
      },
      "source": [
        "What one would normally start with in any machine learning pipeline is writing a dataset - a class that will fetch the training samples. In the previous exercises we did not have to worry about it, since we used the classic datasets available in the torchvision library. However, once you switch to using your own data, you would have to figure out how to fetch the data yourself. Luckily most of the functionality is already provided, but what you need to do is to write a class, that will actually supply the dataloader with training samples - a Dataset.\n",
        "\n",
        "For this exercise you will not have to do it yourself yet, but please carefully read through the provided class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FsSW56Rh7ZwA"
      },
      "source": [
        "# Set some parameters\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 3\n",
        "TRAIN_PATH = 'nuclei_train_data/'\n",
        "TEST_PATH = 'nuclei_val_data/'\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0sGsnkG0GB-n"
      },
      "source": [
        "def get_data(path, train=True):\n",
        "  # get the total number of samples\n",
        "  ids = next(os.walk(path))[1]\n",
        "  X = np.zeros((len(ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "  Y = np.zeros((len(ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
        "  print('Getting and resizing train images and masks ... ')\n",
        "  sys.stdout.flush()\n",
        "  for n, id_ in tqdm(enumerate(ids), total=len(ids)):\n",
        "    path_new = path + id_\n",
        "    # we'll be using skimage library for reading file\n",
        "    img = imread(path_new + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
        "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
        "    X[n] = img\n",
        "    # masks directory has multiple images - one mask per nucleus\n",
        "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
        "    for mask_file in next(os.walk(path_new + '/masks/'))[2]:\n",
        "        mask_ = imread(path_new + '/masks/' + mask_file)\n",
        "        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
        "                                      preserve_range=True), axis=-1)\n",
        "        mask = np.maximum(mask, mask_)\n",
        "    Y[n] = mask\n",
        "  if train:\n",
        "    return X, Y\n",
        "  else:\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bAA6AwnUjqp"
      },
      "source": [
        "Now let's load the dataset and visualize it by calling our function:\n",
        "\n",
        "In this example, we read all images of the train folder as training data (applied SGD on) and all images of the validation folder for testing data (report performance on). Validation data (optimize hyper-parameters on) will be taken randomly from training data during the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jda9EBbmHUNy"
      },
      "source": [
        "X_train, Y_train = get_data(TRAIN_PATH, train=True)\n",
        "X_test, Y_test = get_data(TEST_PATH, train=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iyve1KK4-sS8"
      },
      "source": [
        "# Check if training data looks all right\n",
        "ix = random.randint(0, len(X_train))\n",
        "imshow(X_train[ix])\n",
        "plt.show()\n",
        "imshow(np.squeeze(Y_train[ix]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ZjLuyGXXPL"
      },
      "source": [
        "## Building a U-NET model\n",
        "Now we need to define the architecture of the model to use. This time we will use a [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) that has proven to steadily outperform the other architectures in segmenting biological and medical images.\n",
        "\n",
        "The U-net has an encoder-decoder structure: \n",
        "\n",
        "In the encoder pass, the input image is successively downsampled via max-pooling. In the decoder pass it is upsampled again via transposed convolutions.\n",
        "\n",
        "In adddition, it has skip connections, that bridge the output from an encoder to the corresponding decoder.\n",
        "\n",
        "Note that we are using valid convolutions here; the input to convolutions are not padded and the spatial output size after applying them decreases. Hence, the spatial output size of the network will be smaller than the spatial input size. This could be avoided by using same convolutions, which would increase the computational effort though.\n",
        "\n",
        "Compared to the paper, we will use less features (channels) to enable training the network on the CPU as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FG7qWUaZDGX7"
      },
      "source": [
        "#Each block of u-net architecture consist of two Convolution layers\n",
        "# These two layers are written in a function to make our code clean\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
        "    # first layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size),\n",
        "               padding=\"same\")(input_tensor)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    # second layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), \n",
        "               padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lmmcdboyDOfy"
      },
      "source": [
        "# The u-net architecture consists of contracting and expansive paths which\n",
        "# shrink and expands the inout image respectivly. \n",
        "# Output image have the same size of input image\n",
        "def get_unet(input_img, n_filters):\n",
        "    # contracting path\n",
        "    c1 = conv2d_block(input_img, n_filters=n_filters*4, kernel_size=3) #The first block of U-net\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "    c2 = conv2d_block(p1, n_filters=n_filters*8, kernel_size=3)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "    c3 = conv2d_block(p2, n_filters=n_filters*16, kernel_size=3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "    c4 = conv2d_block(p3, n_filters=n_filters*32, kernel_size=3)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "    \n",
        "    c5 = conv2d_block(p4, n_filters=n_filters*64, kernel_size=3) # last layer on encoding path \n",
        "    \n",
        "    # expansive path\n",
        "    u6 = Conv2DTranspose(n_filters*32, (3, 3), strides=(2, 2), padding='same') (c5) #upsampling included\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = conv2d_block(u6, n_filters=n_filters*32, kernel_size=3)\n",
        "\n",
        "    u7 = Conv2DTranspose(n_filters*16, (3, 3), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = conv2d_block(u7, n_filters=n_filters*16, kernel_size=3)\n",
        "\n",
        "    u8 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = conv2d_block(u8, n_filters=n_filters*8, kernel_size=3)\n",
        "\n",
        "    u9 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = conv2d_block(u9, n_filters=n_filters*4, kernel_size=3)\n",
        "    \n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "    model = Model(inputs=[input_img], outputs=[outputs])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjAen-hlbLdb"
      },
      "source": [
        "## Loss and distance metrics\n",
        "\n",
        "The next step to do would be writing a loss function - a metric that will tell us how close we are to the desired output. This metric should be differentiable, since this is the value to be backpropagated. The are [multiple losses](https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/) we could use for the segmentation task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLBwBMipbLdl"
      },
      "source": [
        "We use Binary Cross Entropy averaged over pixels as training loss.\n",
        "This loss function is similar to the cross entropy loss we have used\n",
        "for the previous classification tasks.\n",
        "\n",
        "The difference to these tasks is that we predict a single number per pixel\n",
        "(the probability of this pixel being foreground / background) instead of \n",
        "a vector per image that encodes the probabilities for several classes.\n",
        "\n",
        "We also will use the [Dice Coefficeint](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to evaluate the network predictions.\n",
        "We can use it for validation if we interpret set $a$ as predictions and $b$ as labels. It is often used to evaluate segmentations with sparse foreground, because the denominator normalizes by the number of foreground pixels.\n",
        "The Dice Coefficient is closely related to Jaccard Index / Intersection over Union."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s2ypLh2eDaFg"
      },
      "source": [
        "# the coefficient takes values in [0, 1], where 0 is the worst score, 1 is the best score\n",
        "# the dice coefficient of two sets represented as vectors a, b ca be computed as (2 *|a b| / (a^2 + b^2))\n",
        "def dice_coefficient(y_true, y_pred):\n",
        "    eps = 1e-6\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f) + eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6C5sXunADfs-"
      },
      "source": [
        "# Creating and Compiling the model\n",
        "input_img = Input((X_train.shape[1], X_train.shape[2], 3), name='img')\n",
        "model = get_unet(input_img, n_filters=4)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[dice_coefficient])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dJ7o-MLtESE4"
      },
      "source": [
        "# saving the log and show it by tensorboard\n",
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AESOWsYnEZCt"
      },
      "source": [
        "# Fiting the model \n",
        "results = model.fit(X_train, Y_train, \n",
        "                    batch_size=5, epochs=15, \n",
        "                    callbacks=[TensorBoardColabCallback(tbc)],\n",
        "                    validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKo4jqpYeHLK"
      },
      "source": [
        "## Model testing and predictions\n",
        "Now this is the time to evaluate our training model on test data which the model has never seen them before. In Keras, we can use \"model.evaluate\" to evaluate the training model where there is an avalibility of masks of test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jse1rUxRSQr3"
      },
      "source": [
        "model.evaluate(X_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L17xt0U-ehA9"
      },
      "source": [
        "In Keras, \"model.predict\" is the function to predict output (masks in segmentation task or labels in classification task). Then we visualize results and visually compare the predicted masks with the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nMiuCBWAFJFI"
      },
      "source": [
        "preds_test = model.predict(X_test, verbose=1)\n",
        "# we apply a threshold on predicted mask (probability mask) to convert it to a binary mask.\n",
        "preds_test_t = (preds_test > 0.3).astype(np.uint8) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ILtSa0-OQYMo"
      },
      "source": [
        "ix = random.randint(0, len(X_test))\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_test[ix,:,:,0])\n",
        "plt.title(\"input image\")\n",
        "plt.subplot(222)\n",
        "plt.imshow(np.squeeze(Y_test[ix, :, :, 0]))\n",
        "plt.title(\"ground truth\")\n",
        "plt.subplot(223)\n",
        "plt.imshow(np.squeeze(preds_test[ix, :, :, 0]))\n",
        "plt.title(\"Probability map of the predicted mask\")\n",
        "plt.subplot(224)\n",
        "plt.imshow(np.squeeze(preds_test_t[ix, :, :, 0]))\n",
        "plt.title(\"Predicted mask after thresholding\")\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9kGHGgpeNT"
      },
      "source": [
        "## Additional Exercises \n",
        "\n",
        "1. Implement and compare at least 2 of the following architecture variants of the U-Net:\n",
        "    * use [Dropout](https://keras.io/layers/core/) in the decoder path\n",
        "    * use [BatchNorm](https://keras.io/layers/normalization/) to normalize layer inputs\n",
        "    * use [ELU-Activations](https://keras.io/layers/advanced-activations/) instead of ReLU-Activations\n",
        "\n",
        "2. Add one more layer to the Unet model (currently it has 4). Compare the results."
      ]
    }
  ]
}